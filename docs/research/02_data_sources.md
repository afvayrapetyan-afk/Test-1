# Research: Data Sources –¥–ª—è Trend Discovery

**–î–∞—Ç–∞**: 2026-01-13
**–°—Ç–∞—Ç—É—Å**: ‚úÖ Completed

## Summary

–ò–∑—É—á–∏–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ç—Ä–µ–Ω–¥–æ–≤ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã. –û–ø—Ä–µ–¥–µ–ª–∏–ª–∏ API, –º–µ—Ç–æ–¥—ã –¥–æ—Å—Ç—É–ø–∞ –∏ best practices –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π, –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º –∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–æ–≤.

---

## –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –ò—Å—Ç–æ—á–Ω–∏–∫–∏

### 1. Twitter/X API

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**:
- Real-time stream trending topics
- Search recent tweets (last 7 days)
- Trending hashtags –ø–æ –≥–µ–æ–ª–æ–∫–∞—Ü–∏–∏
- Engagement metrics (likes, retweets, replies)

**API Endpoints**:
```python
# Twitter API v2
import tweepy

client = tweepy.Client(bearer_token=BEARER_TOKEN)

# Get trending topics
trends = client.get_place_trends(id=1)  # Worldwide

# Search recent tweets
tweets = client.search_recent_tweets(
    query="startup OR SaaS",
    max_results=100,
    tweet_fields=['created_at', 'public_metrics']
)
```

**Rate Limits**:
- Free tier: 500k tweets/month
- Basic: $100/mo –¥–ª—è 1M tweets
- Pro: $5000/mo –¥–ª—è 10M tweets

**Pros**: –í—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å —Ç—Ä–µ–Ω–¥–æ–≤, –±–æ–ª—å—à–æ–π –æ—Ö–≤–∞—Ç
**Cons**: –î–æ—Ä–æ–≥–æ –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–µ, —Ç—Ä–µ–±—É–µ—Ç –º–æ–¥–µ—Ä–∞—Ü–∏—é (–º–Ω–æ–≥–æ —à—É–º–∞)

---

### 2. Reddit API (PRAW)

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**:
- Hot/Rising/Top posts –ø–æ subreddits
- Comment analysis
- Cross-post tracking
- Award tracking (–ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞)

**API Implementation**:
```python
import praw

reddit = praw.Reddit(
    client_id=CLIENT_ID,
    client_secret=CLIENT_SECRET,
    user_agent="TrendScout/1.0"
)

# Monitor trending subreddits
for subreddit_name in ['entrepreneur', 'startups', 'SaaS']:
    subreddit = reddit.subreddit(subreddit_name)

    for post in subreddit.hot(limit=100):
        # Analyze engagement
        trend_score = post.score + post.num_comments * 2
```

**Rate Limits**:
- 60 requests/minute
- OAuth required

**Pros**: –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∏—Å–∫—É—Å—Å–∏–π, early signals
**Cons**: –ú–µ–Ω—å—à–∏–π –º–∞—Å—à—Ç–∞–± —á–µ–º Twitter

---

### 3. Google Trends API (pytrends)

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**:
- Interest over time
- Related queries
- Rising queries (fast-growing)
- Geographic distribution

**Implementation**:
```python
from pytrends.request import TrendReq

pytrends = TrendReq(hl='en-US', tz=360)

# Build payload
pytrends.build_payload(
    kw_list=['AI agents', 'automation'],
    timeframe='now 7-d',
    geo='',  # Worldwide or 'US', 'RU'
)

# Get rising queries (best for trends)
rising = pytrends.related_queries()
interest_over_time = pytrends.interest_over_time()
```

**Rate Limits**:
- Unofficial API (—á–µ—Ä–µ–∑ pytrends)
- ~1 request/second —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
- Rotate proxies –¥–ª—è –º–∞—Å—à—Ç–∞–±–∞

**Pros**: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ Google, –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç macro trends
**Cons**: –ù–µ–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API, –º–æ–∂–µ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å

---

### 4. Product Hunt API

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**:
- Daily featured products
- Upvote tracking
- Category trends
- Maker profiles

**Implementation**:
```python
import requests

headers = {
    'Authorization': f'Bearer {PRODUCT_HUNT_TOKEN}'
}

# Get today's featured products
response = requests.get(
    'https://api.producthunt.com/v2/api/graphql',
    headers=headers,
    json={
        'query': '''
        {
          posts(order: VOTES, postedAfter: "2026-01-13") {
            edges {
              node {
                name
                tagline
                votesCount
                topics { edges { node { name } } }
              }
            }
          }
        }
        '''
    }
)
```

**Rate Limits**:
- 500 requests/day (free)
- GraphQL API

**Pros**: –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ startup –∏–¥–µ–∏, high signal
**Cons**: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π scope (—Ç–æ–ª—å–∫–æ tech products)

---

## –†–æ—Å—Å–∏–π—Å–∫–∏–µ –ò—Å—Ç–æ—á–Ω–∏–∫–∏

### 1. –Ø–Ω–¥–µ–∫—Å Wordstat API

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**:
- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
- –î–∏–Ω–∞–º–∏–∫–∞ —Å–ø—Ä–æ—Å–∞ (trends over time)
- –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–æ–≤
- –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞
- –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑

**–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ API** (–±–µ—Ç–∞ —Å 2025):
```python
import requests

# –Ø–Ω–¥–µ–∫—Å Wordstat API (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π)
API_KEY = "your_yandex_api_key"

response = requests.post(
    'https://api.direct.yandex.com/json/v5/keywordsresearch',
    headers={
        'Authorization': f'Bearer {API_KEY}',
        'Accept-Language': 'ru'
    },
    json={
        'method': 'get',
        'params': {
            'Keywords': ['—Å—Ç–∞—Ä—Ç–∞–ø', 'AI –∞–≥–µ–Ω—Ç—ã', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è'],
            'RegionIds': [225],  # –†–æ—Å—Å–∏—è
        }
    }
)

# –ü–æ–ª—É—á–∞–µ–º frequency, competition, trends
data = response.json()
```

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ - –ü–∞—Ä—Å–∏–Ω–≥** (—á–µ—Ä–µ–∑ —Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã):
- arsenkin.ru/tools/wordstat/ (–ø–∞—Ä—Å–∏–Ω–≥ –æ–Ω–ª–∞–π–Ω)
- –ò—Å–ø–æ–ª—å–∑—É—è Selenium –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏

**Rate Limits**:
- API: –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–∞—Ä–∏—Ñ–∞ –Ø–Ω–¥–µ–∫—Å.–î–∏—Ä–µ–∫—Ç
- Parsing: ~10-20 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω—É—Ç—É (risk of ban)

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
- –û—Ü–µ–Ω–∫–∞ —Å–ø—Ä–æ—Å–∞ –Ω–∞ —Ç–æ–≤–∞—Ä—ã/—É—Å–ª—É–≥–∏
- –í—ã—è–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤
- –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–∞–∂ —Å —É—á—ë—Ç–æ–º —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏
- –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑

**Pros**: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ø–Ω–¥–µ–∫—Å–∞, –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–æ—Å—Å–∏–π—Å–∫–∏–π —Ä—ã–Ω–æ–∫
**Cons**: –¢—Ä–µ–±—É–µ—Ç –∞–∫–∫–∞—É–Ω—Ç –Ø–Ω–¥–µ–∫—Å.–î–∏—Ä–µ–∫—Ç –¥–ª—è API, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –¥–æ—Å—Ç—É–ø

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏**:
- [–Ø–Ω–¥–µ–∫—Å –í–æ—Ä–¥—Å—Ç–∞—Ç –æ–±–∑–∞–≤—ë–ª—Å—è API](https://yandex.ru/company/news/01-09-06-2025)
- [API –í–æ—Ä–¥—Å—Ç–∞—Ç–∞ - –ü–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ](https://osipenkov.ru/api-wordstat/)
- [–ü–∞—Ä—Å–∏–Ω–≥ –Ø–Ω–¥–µ–∫—Å Wordstat –æ–Ω–ª–∞–π–Ω](https://arsenkin.ru/tools/wordstat/)

---

### 2. VK API (–í–ö–æ–Ω—Ç–∞–∫—Ç–µ)

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**:
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ—Å—Ç–æ–≤ –≤ –≥—Ä—É–ø–ø–∞—Ö/–ø–∞–±–ª–∏–∫–∞—Ö
- Trending hashtags
- Engagement metrics (–ª–∞–π–∫–∏, —Ä–µ–ø–æ—Å—Ç—ã, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏)
- –ù–æ–≤–æ—Å—Ç–Ω–∞—è –ª–µ–Ω—Ç–∞ –ø–æ —Ç–µ–º–∞–º
- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ–æ–±—â–µ—Å—Ç–≤

**API Methods**:
```python
import vk_api

vk_session = vk_api.VkApi(token=VK_ACCESS_TOKEN)
vk = vk_session.get_api()

# 1. –ü–æ–∏—Å–∫ —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ (newsfeed.search)
trending_posts = vk.newsfeed.search(
    q='—Å—Ç–∞—Ä—Ç–∞–ø OR –±–∏–∑–Ω–µ—Å',
    count=100,
    extended=1
)

# 2. –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ –∏–∑ –≥—Ä—É–ø–ø—ã (wall.get)
group_posts = vk.wall.get(
    owner_id=-123456789,  # ID –≥—Ä—É–ø–ø—ã
    count=100,
    filter='all'
)

# 3. –ê–Ω–∞–ª–∏–∑ engagement
for post in group_posts['items']:
    engagement = (
        post['likes']['count'] * 1 +
        post['reposts']['count'] * 3 +
        post['comments']['count'] * 2
    )
```

**Automation —Å n8n**:
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ VK —Ç—Ä–µ–Ω–¥–æ–≤ —á–µ—Ä–µ–∑ AI (newsfeed.search + –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–º)
- Auto-drafts –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ—Å—Ç–∏–Ω–≥–∞
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Google Sheets, CRM

**–°–µ—Ä–≤–∏—Å—ã –∞–Ω–∞–ª–∏—Ç–∏–∫–∏**:
- **Popsters** - –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ engagement –ø–æ—Å—Ç–æ–≤
- **SocStat** - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä—É–ø–ø –í–ö–æ–Ω—Ç–∞–∫—Ç–µ
- **SMMPLANNER** - –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞

**Rate Limits**:
- 3 requests/second
- 5000 requests/day (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω)
- 100000 requests/day (server-side token)

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
- –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ industry trends
- –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ optimal posting times
- Engagement analysis

**Pros**: –ö—Ä—É–ø–Ω–µ–π—à–∞—è —Ä–æ—Å—Å–∏–π—Å–∫–∞—è —Å–æ—Ü—Å–µ—Ç—å, rich API
**Cons**: –¢—Ä–µ–±—É–µ—Ç OAuth, rate limits —Å—Ç—Ä–æ–∂–µ —á–µ–º —É Twitter

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏**:
- [–ö–æ–Ω—Ç–µ–Ω—Ç-–∑–∞–≤–æ–¥ –í–ö–æ–Ω—Ç–∞–∫—Ç–µ: –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å n8n –∏ VK API](https://dobromarketing.ru/vk-content-automation-n8n/)
- [–ö–∞–∫ –≤—ã—Ç—è–Ω—É—Ç—å –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ API –í–ö](https://habr.com/ru/articles/662858/)
- [Popsters - –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –í–ö–æ–Ω—Ç–∞–∫—Ç–µ](https://popsters.ru/vk/)

---

### 3. Telegram Monitoring

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**:
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—É–±–ª–∏—á–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤
- Trending topics –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- Engagement tracking (views, forwards)
- Keyword mentions
- Sentiment analysis

**–°–µ—Ä–≤–∏—Å—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞**:

#### TGStat (–õ—É—á—à–∏–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏)
```python
import requests

# TGStat API
API_TOKEN = "your_tgstat_token"

# –ü–æ–∏—Å–∫ –ø–æ –∫–∞–Ω–∞–ª–∞–º
response = requests.get(
    'https://api.tgstat.ru/channels/search',
    params={
        'token': API_TOKEN,
        'q': '—Å—Ç–∞—Ä—Ç–∞–ø',
        'limit': 50
    }
)

# Real-time –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (5-10 —Å–µ–∫—É–Ω–¥ –ø–æ—Å–ª–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏)
# Webhook –¥–ª—è –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤
response = requests.get(
    'https://api.tgstat.ru/posts/search',
    params={
        'token': API_TOKEN,
        'q': 'AI OR –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è',
        'extended': 1,
        'peer_type': 'channel'
    }
)

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–Ω–∞–ª–∞
channel_stats = requests.get(
    f'https://api.tgstat.ru/channels/stat',
    params={
        'token': API_TOKEN,
        'channelId': '@channel_username'
    }
)
```

**API Coverage**:
- 2.8M+ Telegram –∫–∞–Ω–∞–ª–æ–≤
- 54.3B+ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
- 30M+ –Ω–æ–≤—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π –≤ –¥–µ–Ω—å
- Real-time delivery (5-10 —Å–µ–∫)

#### Telemetr
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–æ—Å—Ç–∞ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤
- Engagement –∞–Ω–∞–ª–∏–∑
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–µ–∫–ª–∞–º—ã
- –ü–æ–∏—Å–∫ –ø–ª–æ—â–∞–¥–æ–∫ –¥–ª—è —Ä–∞–∑–º–µ—â–µ–Ω–∏—è

#### –ú–µ–¥–∏–∞–ª–æ–≥–∏—è (Medialogia)
- Social media monitoring
- Sentiment analysis
- Reach metrics
- Message engagement
- Unlimited queries (–ø–ª–∞—Ç–Ω—ã–π —Ç–∞—Ä–∏—Ñ)

**Telegram Bot API** (–¥–ª—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –±–æ—Ç–∞):
```python
from telethon import TelegramClient

# Telethon –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞–Ω–∞–ª–æ–≤
client = TelegramClient('session', api_id, api_hash)

async def monitor_channels():
    # –ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ –∫–∞–Ω–∞–ª—ã
    channels = ['@startup_ru', '@biznes_molodost', '@russianvc']

    @client.on(events.NewMessage(chats=channels))
    async def handler(event):
        # Analyze new post
        engagement_score = event.message.views or 0

        # Save to database
        await save_trend({
            'text': event.message.text,
            'views': event.message.views,
            'date': event.message.date,
            'channel': event.chat.title
        })
```

**Rate Limits**:
- TGStat API: –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–∞—Ä–∏—Ñ–∞ (–æ—Ç $50/mo)
- Telegram Bot API: 30 messages/second

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
- Tracking brand mentions
- Competitor monitoring
- Trend detection (viral posts)
- Keyword alerts
- Advertising campaign analysis

**Pros**: Telegram - –≥–ª–∞–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ –≤ –†–§, rich analytics APIs
**Cons**: –¢—Ä–µ–±—É–µ—Ç –ø–ª–∞—Ç–Ω—ã–µ API –¥–ª—è –º–∞—Å—à—Ç–∞–±–∞ (TGStat ~$50-200/mo)

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏**:
- [TGStat - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Telegram](https://tgstat.ru/en/alerts)
- [TGStat API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](https://tgstat.ru/en/api/stat)
- [–¢–û–ü-20 —Å–µ—Ä–≤–∏—Å–æ–≤ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –¢–µ–ª–µ–≥—Ä–∞–º –∫–∞–Ω–∞–ª–æ–≤](https://vc.ru/telegram/1461828-analitika-telegramm-kanalov-top-20-servisov-i-botov-dlya-analiza-tg-kanalov)
- [Telemetr - –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ Telegram](https://telemetr.me/)

---

### 4. –†–æ—Å—Å–∏–π—Å–∫–∏–µ –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –ê–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏**:
- **–Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏** - –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π
- **Lenta.ru**, **RBC.ru**, **Vedomosti.ru** - RSS feeds
- **Habr** - IT/tech —Ç—Ä–µ–Ω–¥—ã (–µ—Å—Ç—å API)

**Habr API Example**:
```python
import requests

# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
response = requests.get(
    'https://habr.com/ru/rss/all/',
    headers={'User-Agent': 'TrendScout/1.0'}
)

# –ü–∞—Ä—Å–∏–Ω–≥ RSS
import feedparser
feed = feedparser.parse(response.text)

for entry in feed.entries:
    # Analyze tech trends
    if entry.published_parsed > recent_date:
        trends.append({
            'title': entry.title,
            'link': entry.link,
            'tags': entry.tags,
            'published': entry.published
        })
```

---

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –°–µ—Ç–∏

### 5. YouTube Data API

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**:
- Trending videos –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
- Channel analytics & growth
- Video engagement metrics (views, likes, comments)
- Keyword search –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö/–æ–ø–∏—Å–∞–Ω–∏—è—Ö
- Related videos clustering

**API Implementation**:
```python
from googleapiclient.discovery import build

youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)

# Get trending videos
trending_request = youtube.videos().list(
    part='snippet,statistics',
    chart='mostPopular',
    regionCode='RU',  # or 'US'
    maxResults=50,
    videoCategoryId='28'  # Science & Technology
)

trending_videos = trending_request.execute()

# Search for specific topics
search_request = youtube.search().list(
    part='snippet',
    q='AI startup OR –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è',
    type='video',
    order='viewCount',  # or 'date', 'relevance'
    publishedAfter='2026-01-01T00:00:00Z',
    maxResults=50
)

# Analyze channels
channel_stats = youtube.channels().list(
    part='statistics,snippet',
    id='UC_channel_id'
).execute()
```

**Rate Limits**:
- 10,000 quota units/day (free tier)
- search.list = 100 units
- videos.list = 1 unit
- ~100 searches/day free

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
- –í–∏–∑—É–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã (—á—Ç–æ –ª—é–¥–∏ —Å–º–æ—Ç—Ä—è—Ç)
- –í–ª–∏—è—Ç–µ–ª—å–Ω—ã–µ –∫–∞–Ω–∞–ª—ã –≤ niche
- Educational content trends
- Product reviews & demos

**Pros**: –û–≥—Ä–æ–º–Ω—ã–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö, –≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç interest
**Cons**: Quota limits —Å—Ç—Ä–æ–≥–∏–µ, –Ω—É–∂–µ–Ω API key

---

### 6. Instagram (Meta Graph API)

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**:
- Hashtag trends
- Business account insights
- Post engagement (likes, comments, saves)
- Stories metrics
- Influencer tracking

**API Implementation** (—Ç—Ä–µ–±—É–µ—Ç Business account):
```python
import requests

# Instagram Graph API
access_token = "YOUR_ACCESS_TOKEN"
instagram_business_account_id = "YOUR_ID"

# Get media insights
url = f"https://graph.facebook.com/v18.0/{instagram_business_account_id}/media"
params = {
    'fields': 'id,caption,media_type,media_url,permalink,timestamp,like_count,comments_count',
    'access_token': access_token
}

response = requests.get(url, params=params)
media = response.json()

# Hashtag search (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø)
hashtag_url = f"https://graph.facebook.com/v18.0/ig_hashtag_search"
hashtag_params = {
    'user_id': instagram_business_account_id,
    'q': 'startup',  # hashtag
    'access_token': access_token
}
```

**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è**:
- –¢—Ä–µ–±—É–µ—Ç—Å—è Business/Creator account
- –î–æ—Å—Ç—É–ø —Ç–æ–ª—å–∫–æ –∫ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º + tagged content
- Hashtag search –æ–≥—Ä–∞–Ω–∏—á–µ–Ω
- Rate limits: 200 calls/hour

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ - Scraping** (risk of ban):
- Instaloader library (–Ω–µ–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π)
- Apify Instagram Scraper

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
- Visual trends (—á—Ç–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ –≤–∏–∑—É–∞–ª—å–Ω–æ)
- Influencer marketing opportunities
- Brand mentions
- Product launch tracking

**Pros**: –í–∏–∑—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, influencer insights
**Cons**: –û—á–µ–Ω—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π API, —Ç—Ä–µ–±—É–µ—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏

---

### 7. Facebook Graph API

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**:
- Public page posts
- Group discussions (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø)
- Events
- Reactions, comments, shares

**API Implementation**:
```python
import facebook

graph = facebook.GraphAPI(access_token=FB_ACCESS_TOKEN)

# Get public page posts
posts = graph.get_connections(
    id='page_id',
    connection_name='posts',
    fields='message,created_time,likes.summary(true),comments.summary(true),shares'
)

# Search public posts (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø)
search = graph.search(
    type='page',
    q='startup funding',
    fields='name,fan_count,category'
)
```

**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è**:
- –î–æ—Å—Ç—É–ø –∫ public –¥–∞–Ω–Ω—ã–º —Å–∏–ª—å–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω –ø–æ—Å–ª–µ Cambridge Analytica
- Groups API —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
- Rate limits —Å—Ç—Ä–æ–≥–∏–µ

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ** (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ):
- Public page monitoring
- Event discovery
- Brand mentions –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö

**Pros**: –û–≥—Ä–æ–º–Ω–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è
**Cons**: –û—á–µ–Ω—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π API, —Å–ª–æ–∂–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π API. –õ—É—á—à–µ —Ñ–æ–∫—É—Å –Ω–∞ Reddit, YouTube, Telegram.

---

## –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ & Startup –ü–æ—Ä—Ç–∞–ª—ã

### 8. Crunchbase API

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**:
- –ù–æ–≤—ã–µ —Å—Ç–∞—Ä—Ç–∞–ø—ã –∏ –∏—Ö –æ–ø–∏—Å–∞–Ω–∏—è
- –†–∞—É–Ω–¥—ã —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è
- Investors & VCs
- Acquisitions & IPOs
- Company categories & trends

**API Implementation**:
```python
import requests

CRUNCHBASE_API_KEY = "your_api_key"

# Search for recently funded startups
url = "https://api.crunchbase.com/api/v4/searches/organizations"
headers = {
    'X-cb-user-key': CRUNCHBASE_API_KEY,
    'Content-Type': 'application/json'
}

payload = {
    "field_ids": [
        "identifier",
        "name",
        "short_description",
        "categories",
        "funding_total",
        "last_funding_at",
        "founded_on"
    ],
    "order": [
        {
            "field_id": "last_funding_at",
            "sort": "desc"
        }
    ],
    "query": [
        {
            "type": "predicate",
            "field_id": "last_funding_at",
            "operator_id": "gte",
            "values": ["2026-01-01"]
        }
    ],
    "limit": 100
}

response = requests.post(url, json=payload, headers=headers)
startups = response.json()

# Get funding rounds
funding_url = "https://api.crunchbase.com/api/v4/searches/funding_rounds"
# Similar structure
```

**Pricing**:
- Free tier: –Ω–µ—Ç
- Basic: $29/mo (limited API calls)
- Pro: $99/mo (1000 calls/month)
- Enterprise: Custom pricing

**Rate Limits**:
- Basic: 200 calls/day
- Pro: 1000 calls/month

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
- Tracking –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤ –≤ specific sectors
- –ê–Ω–∞–ª–∏–∑ funding trends (–∫–∞–∫–∏–µ –∏–¥–µ–∏ –ø–æ–ª—É—á–∞—é—Ç –¥–µ–Ω—å–≥–∏)
- Competitor monitoring
- Investment thesis validation

**Pros**: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–∏, comprehensive
**Cons**: –î–æ—Ä–æ–≥–æ ($99/mo –º–∏–Ω–∏–º—É–º –¥–ª—è API), rate limits

---

### 9. AngelList/Wellfound API

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**:
- Startup listings
- Jobs at startups (–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫–∏–µ —Å—Ç–∞—Ä—Ç–∞–ø—ã —Ä–∞—Å—Ç—É—Ç)
- Investor profiles
- Startup valuations

**API Status**:
‚ö†Ô∏è AngelList –∑–∞–∫—Ä—ã–ª –ø—É–±–ª–∏—á–Ω—ã–π API –≤ 2023 –≥–æ–¥—É. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã:
- Wellfound (–Ω–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ) - –Ω–µ—Ç –ø—É–±–ª–∏—á–Ω–æ–≥–æ API
- Scraping (—Ä–∏—Å–∫ –±–∞–Ω–∞)

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥**:
```python
# Web scraping (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ)
import requests
from bs4 import BeautifulSoup

def scrape_wellfound_jobs():
    """Scrape job listings as proxy for startup growth"""
    url = "https://wellfound.com/role/r/software-engineer"

    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
    soup = BeautifulSoup(response.text, 'html.parser')

    # Parse listings
    # Identify growing startups by # of open positions
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
- Proxy metric: –º–Ω–æ–≥–æ –≤–∞–∫–∞–Ω—Å–∏–π = —Ä–∞—Å—Ç—É—â–∏–π —Å—Ç–∞—Ä—Ç–∞–ø
- Industry trends (AI, crypto, SaaS)
- Talent market analysis

**Pros**: –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π growth —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤
**Cons**: –ù–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ API, —Ç–æ–ª—å–∫–æ scraping

---

### 10. Product Hunt (—É–∂–µ –æ–ø–∏—Å–∞–Ω –≤—ã—à–µ)

–û—Å—Ç–∞–µ—Ç—Å—è –≤ Tier 1 –¥–ª—è daily startup launches.

---

## –ù–æ–≤–æ—Å—Ç–Ω—ã–µ & –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –ü–æ—Ä—Ç–∞–ª—ã

### 11. TechCrunch RSS & API

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**:
- Tech news feed
- Startup launches
- Funding announcements
- Trend analysis articles

**RSS Implementation**:
```python
import feedparser

# TechCrunch RSS feeds
feeds = {
    'main': 'https://techcrunch.com/feed/',
    'startups': 'https://techcrunch.com/category/startups/feed/',
    'ai': 'https://techcrunch.com/category/artificial-intelligence/feed/',
    'funding': 'https://techcrunch.com/tag/funding/feed/'
}

for category, url in feeds.items():
    feed = feedparser.parse(url)

    for entry in feed.entries:
        article = {
            'title': entry.title,
            'link': entry.link,
            'published': entry.published,
            'summary': entry.summary,
            'categories': [tag.term for tag in entry.tags]
        }
```

**Rate Limits**:
- RSS: unlimited (politeness recommended)
- Scraping: 1 request/second

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
- Breaking startup news
- Funding announcements (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Crunchbase)
- Industry trends
- Product launches

**Pros**: Free, high-quality content, early signals
**Cons**: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –Ω—É–∂–µ–Ω –¥–ª—è extracting insights

---

### 12. Habr + VC.ru (–†–æ—Å—Å–∏–π—Å–∫–∏–µ)

**Habr API/RSS**:
```python
# Habr RSS
habr_feeds = {
    'all': 'https://habr.com/ru/rss/all/',
    'top': 'https://habr.com/ru/rss/top/',
    'hubs/startup': 'https://habr.com/ru/hub/startup/rss/',
}

feed = feedparser.parse(habr_feeds['hubs/startup'])
```

**VC.ru Scraping**:
```python
# VC.ru –Ω–µ –∏–º–µ–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ API
# –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è RSS –∏–ª–∏ scraping
vc_url = "https://vc.ru/feed"

response = requests.get(vc_url)
soup = BeautifulSoup(response.text, 'html.parser')

# –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π –ø—Ä–æ —Å—Ç–∞—Ä—Ç–∞–ø—ã, –≤–µ–Ω—á—É—Ä, –±–∏–∑–Ω–µ—Å
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
- –†–æ—Å—Å–∏–π—Å–∫–∏–π tech landscape
- Local startup ecosystem
- Ven—Ç—É trends –≤ –†–§

**Pros**: Free, —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
**Cons**: –ù–µ—Ç API, –Ω—É–∂–µ–Ω scraping/RSS

---

## –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¢–∞–±–ª–∏—Ü–∞ –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤

### –ü–æ–∏—Å–∫–æ–≤—ã–µ –°–∏—Å—Ç–µ–º—ã (–ü–†–ò–û–†–ò–¢–ï–¢ 1)
| –ò—Å—Ç–æ—á–Ω–∏–∫ | –û—Ö–≤–∞—Ç | API –ö–∞—á–µ—Å—Ç–≤–æ | Cost/Month | Update Speed | Best For |
|----------|-------|--------------|------------|--------------|----------|
| **Google Trends** | üåç Global | ‚≠ê‚≠ê‚≠ê | Free* | Daily/Weekly | Macro search trends |
| **–Ø–Ω–¥–µ–∫—Å Wordstat** | üá∑üá∫ Russia | ‚≠ê‚≠ê‚≠ê | $50-100 (API) | Weekly | RU search demand |

### –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –°–µ—Ç–∏ (–ü–†–ò–û–†–ò–¢–ï–¢ 2)
| –ò—Å—Ç–æ—á–Ω–∏–∫ | –û—Ö–≤–∞—Ç | API –ö–∞—á–µ—Å—Ç–≤–æ | Cost/Month | Update Speed | Best For |
|----------|-------|--------------|------------|--------------|----------|
| **Reddit** | üåç Global | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Free | Hourly | Niche communities, early signals |
| **YouTube** | üåç Global | ‚≠ê‚≠ê‚≠ê‚≠ê | Free (10k quota/day) | Daily/Hourly | Video trends, educational |
| **Telegram (TGStat)** | üá∑üá∫ Russia | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | $50-200 | Real-time (5s) | Viral content RU |
| **VK** | üá∑üá∫ Russia | ‚≠ê‚≠ê‚≠ê‚≠ê | Free | Hourly | Social trends RU |
| **Instagram** | üåç Global | ‚≠ê‚≠ê | Free (limited) | Daily | Visual trends, influencers |
| **Facebook** | üåç Global | ‚≠ê | Free (very limited) | N/A | Low priority (API –æ–≥—Ä–∞–Ω–∏—á–µ–Ω) |
| **Twitter/X** | üåç Global | ‚≠ê‚≠ê‚≠ê‚≠ê | $100-5000 | Real-time | Breaking news (–¥–æ—Ä–æ–≥–æ) |

### –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ & Startup –ü–æ—Ä—Ç–∞–ª—ã (–ü–†–ò–û–†–ò–¢–ï–¢ 3)
| –ò—Å—Ç–æ—á–Ω–∏–∫ | –û—Ö–≤–∞—Ç | API –ö–∞—á–µ—Å—Ç–≤–æ | Cost/Month | Update Speed | Best For |
|----------|-------|--------------|------------|--------------|----------|
| **Crunchbase** | üåç Global | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | $99-500 | Daily | Funding rounds, new startups |
| **AngelList/Wellfound** | üåç Global | ‚ùå | Free (scraping) | Daily | Startup jobs, growth signals |
| **Product Hunt** | üåç Tech | ‚≠ê‚≠ê‚≠ê‚≠ê | Free | Daily | Daily startup launches |

### –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –ü–æ—Ä—Ç–∞–ª—ã (–ü–†–ò–û–†–ò–¢–ï–¢ 4)
| –ò—Å—Ç–æ—á–Ω–∏–∫ | –û—Ö–≤–∞—Ç | API –ö–∞—á–µ—Å—Ç–≤–æ | Cost/Month | Update Speed | Best For |
|----------|-------|--------------|------------|--------------|----------|
| **TechCrunch** | üåç Global | ‚≠ê‚≠ê‚≠ê (RSS) | Free | Hourly | Tech news, funding announces |
| **Habr** | üá∑üá∫ Russia | ‚≠ê‚≠ê‚≠ê (RSS) | Free | Daily | RU tech trends |
| **VC.ru** | üá∑üá∫ Russia | ‚≠ê (scraping) | Free | Daily | RU business/VC trends |

**–õ–µ–≥–µ–Ω–¥–∞**:
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê = –û—Ç–ª–∏—á–Ω–æ–µ API, –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ, —Ö–æ—Ä–æ—à–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- ‚≠ê‚≠ê‚≠ê‚≠ê = –•–æ—Ä–æ—à–µ–µ API —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
- ‚≠ê‚≠ê‚≠ê = –†–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –µ—Å—Ç—å issues (unofficial, rate limits)
- ‚≠ê‚≠ê = –°–∏–ª—å–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ API
- ‚≠ê = –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ—Ç API, —Ç–æ–ª—å–∫–æ scraping
- ‚ùå = –ù–µ—Ç –ø—É–±–ª–∏—á–Ω–æ–≥–æ API

\* Google Trends - –Ω–µ–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API —á–µ—Ä–µ–∑ pytrends, —Ä–∏—Å–∫ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏

---

## Recommended Architecture –¥–ª—è –Ω–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞

### Data Sources Priority (MVP)

**–ü–†–ò–û–†–ò–¢–ï–¢ 1: –ü–æ–∏—Å–∫–æ–≤—ã–µ –ó–∞–ø—Ä–æ—Å—ã** (–æ—Å–Ω–æ–≤–∞ —Å–∏—Å—Ç–µ–º—ã)
1. **Google Trends** - macro trends, –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
2. **–Ø–Ω–¥–µ–∫—Å Wordstat** - –ø–æ–∏—Å–∫–æ–≤—ã–π —Å–ø—Ä–æ—Å –≤ –†–§

**–ü–†–ò–û–†–ò–¢–ï–¢ 2: –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –°–µ—Ç–∏**
3. **Reddit** - high-quality discussions, early signals
4. **YouTube** - video trends, channel analytics
5. **Telegram (TGStat API)** - real-time —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ —Ç—Ä–µ–Ω–¥—ã
6. **VK API** - —Ä–æ—Å—Å–∏–π—Å–∫–∏–π social proof
7. **Instagram** - –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã, influencer marketing (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
8. **Facebook** - group discussions, events (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

**–ü–†–ò–û–†–ò–¢–ï–¢ 3: –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ & Startup –ü–æ—Ä—Ç–∞–ª—ã**
9. **Crunchbase** - –Ω–æ–≤—ã–µ —Å—Ç–∞—Ä—Ç–∞–ø—ã, —Ä–∞—É–Ω–¥—ã —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è
10. **AngelList/Wellfound** - startup jobs, funding
11. **Product Hunt** - daily startup launches

**–ü–†–ò–û–†–ò–¢–ï–¢ 4: –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –ü–æ—Ä—Ç–∞–ª—ã**
12. **TechCrunch** - tech news, startup launches
13. **TheInformation** - in-depth tech analysis
14. **Habr** - —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ IT —Ç—Ä–µ–Ω–¥—ã
15. **VC.ru** - —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –±–∏–∑–Ω–µ—Å/–≤–µ–Ω—á—É—Ä

**Tier 3 - Future Expansion**:
- Twitter/X (–µ—Å–ª–∏ –±—é–¥–∂–µ—Ç –ø–æ–∑–≤–æ–ª—è–µ—Ç - $100-5000/mo)
- LinkedIn (B2B trends, professional networks)

### Multi-Source Pipeline Architecture

```python
# backend/app/scrapers/multi_source_scraper.py

from typing import List, Dict
import asyncio

class MultiSourceTrendScout:
    def __init__(self):
        self.sources = {
            'google_trends': GoogleTrendsScraper(),
            'tgstat': TelegramScraper(),
            'vk': VKScraper(),
            'reddit': RedditScraper(),
            'yandex_wordstat': YandexWordstatScraper(),
        }

    async def fetch_all_trends(self, keywords: List[str]) -> List[Dict]:
        """Fetch trends from all sources in parallel"""

        tasks = [
            self.sources['google_trends'].scrape(keywords),
            self.sources['tgstat'].scrape(keywords),
            self.sources['vk'].scrape(keywords),
            self.sources['reddit'].scrape(keywords),
        ]

        # Parallel execution
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Merge and deduplicate
        all_trends = []
        for source_result in results:
            if isinstance(source_result, Exception):
                logger.error(f"Source failed: {source_result}")
                continue
            all_trends.extend(source_result)

        # Deduplicate by content similarity
        unique_trends = self._deduplicate_trends(all_trends)

        return unique_trends

    def _deduplicate_trends(self, trends: List[Dict]) -> List[Dict]:
        """Remove duplicate trends using semantic similarity"""
        # Use embeddings to find duplicates
        # Keep the one with highest engagement
        pass
```

### Cost Optimization Strategy

**Monthly Budget Estimate** (MVP):
- Telegram TGStat API: $50-100/mo
- VK API: Free (standard access)
- Google Trends: Free (—Å rate limiting)
- Reddit API: Free
- –Ø–Ω–¥–µ–∫—Å Wordstat: $0 (–ø–∞—Ä—Å–∏–Ω–≥) –∏–ª–∏ $50-100/mo (API)
- **Total**: ~$50-200/mo –¥–ª—è data sources

**–î–ª—è Scale (1000+ –±–∏–∑–Ω–µ—Å–æ–≤)**:
- –î–æ–±–∞–≤–∏—Ç—å Twitter: +$100-500/mo
- Upgrade TGStat: +$100/mo
- –Ø–Ω–¥–µ–∫—Å Wordstat API: +$100/mo
- **Total**: ~$350-800/mo

---

## Best Practices

### 1. Rate Limiting & Politeness
```python
from ratelimit import limits, sleep_and_retry

class PoliteScraper:
    @sleep_and_retry
    @limits(calls=10, period=60)  # 10 calls per minute
    async def fetch_data(self, url: str):
        """Respectful rate limiting"""
        response = await aiohttp.get(url)
        return response
```

### 2. Error Handling & Retries
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
async def fetch_with_retry(self, source: str):
    """Retry on failures"""
    try:
        return await self.sources[source].scrape()
    except Exception as e:
        logger.error(f"Retry failed for {source}: {e}")
        raise
```

### 3. Caching –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ API calls
```python
from functools import lru_cache
import redis

cache = redis.Redis(host='localhost', port=6379, db=0)

def cached_scrape(ttl: int = 3600):
    """Cache scraping results"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            cache_key = f"{func.__name__}:{args}:{kwargs}"

            # Check cache
            cached = cache.get(cache_key)
            if cached:
                return json.loads(cached)

            # Fetch fresh data
            result = await func(*args, **kwargs)

            # Store in cache
            cache.setex(cache_key, ttl, json.dumps(result))

            return result
        return wrapper
    return decorator
```

### 4. Monitoring & Alerts
```python
# Prometheus metrics
from prometheus_client import Counter, Histogram

scrape_requests = Counter(
    'scrape_requests_total',
    'Total scraping requests',
    ['source', 'status']
)

scrape_duration = Histogram(
    'scrape_duration_seconds',
    'Scraping duration',
    ['source']
)

async def monitored_scrape(source: str):
    with scrape_duration.labels(source=source).time():
        try:
            result = await scraper.fetch(source)
            scrape_requests.labels(source=source, status='success').inc()
            return result
        except Exception:
            scrape_requests.labels(source=source, status='error').inc()
            raise
```

---

## Implementation Priority

### Week 1: Foundation
- [ ] Setup Google Trends scraper (pytrends)
- [ ] Setup Reddit API (PRAW)
- [ ] Basic deduplication logic

### Week 2: Russian Sources
- [ ] Integrate TGStat API (Telegram)
- [ ] Setup VK API scraper
- [ ] Test Yandex Wordstat (–ø–∞—Ä—Å–∏–Ω–≥ –∏–ª–∏ API)

### Week 3: Data Pipeline
- [ ] Parallel scraping —Å asyncio
- [ ] Caching layer (Redis)
- [ ] Error handling & retries
- [ ] Prometheus monitoring

### Week 4: Optimization
- [ ] Deduplication —Å embeddings
- [ ] Rate limiting optimization
- [ ] Cost tracking dashboard

---

## Resources & Documentation

### International
- [Twitter API v2 Documentation](https://developer.twitter.com/en/docs/twitter-api)
- [Reddit API (PRAW) Docs](https://praw.readthedocs.io/)
- [pytrends (Google Trends)](https://pypi.org/project/pytrends/)
- [Product Hunt API](https://api.producthunt.com/v2/docs)

### Russian
- [–Ø–Ω–¥–µ–∫—Å Wordstat API](https://osipenkov.ru/api-wordstat/)
- [VK API Documentation](https://dev.vk.com/ru/reference)
- [TGStat API](https://tgstat.ru/en/api/stat)
- [Telemetr](https://telemetr.me/)

### Tools & Libraries
- [Tweepy (Twitter)](https://github.com/tweepy/tweepy)
- [PRAW (Reddit)](https://github.com/praw-dev/praw)
- [vk_api (VK)](https://github.com/python273/vk_api)
- [Telethon (Telegram)](https://github.com/LonamiWebs/Telethon)

---

## Next Steps

1. ‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö (–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ + —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ)
2. ‚è≥ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π API –ª–æ–∫–∞–ª—å–Ω–æ
3. ‚è≥ –ò–∑–º–µ—Ä–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ rate limits
4. ‚è≥ –û—Ü–µ–Ω–∏—Ç—å cost –Ω–∞ 100 trends/day
5. ‚è≥ –°–æ–∑–¥–∞—Ç—å unified scraper interface

