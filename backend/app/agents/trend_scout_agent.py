"""
Trend Scout Agent
Discovers emerging AI trends from various data sources

ðŸŽ¯ Ð¤ÐžÐšÐ£Ð¡: AI-Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸ÐºÐ¸, AI-Ð°Ð³ÐµÐ½Ñ‚Ñ‹, Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ
"""

from typing import Dict, Any, List
import structlog
from sqlalchemy.orm import Session

from app.agents.base_agent import BaseAgent
from app.modules.trends.service import TrendService
from app.modules.trends.schemas import TrendCreate
from app.scrapers.reddit_scraper import RedditScraper

logger = structlog.get_logger()

# AI-focused subreddits for trend discovery
AI_SUBREDDITS = [
    "MachineLearning",
    "artificial",
    "ChatGPT",
    "LocalLLaMA",
    "singularity",
    "SideProject",
    "startups",
    "Entrepreneur",
    "SaaS",
    "nocode",
    "AutomateYourself",
]

# Keywords for filtering AI-related trends
AI_KEYWORDS = [
    "AI", "GPT", "LLM", "agent", "assistant", "bot", "automation",
    "chatbot", "copilot", "Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº", "Ð°Ð³ÐµÐ½Ñ‚", "Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ",
    "Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÑŒ", "Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚",
]


class TrendScoutAgent(BaseAgent):
    """
    Trend Scout Agent - Discovers AI-focused emerging trends

    ðŸŽ¯ Ð¤ÐžÐšÐ£Ð¡:
    - AI-Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸ÐºÐ¸ Ð´Ð»Ñ Ð±Ð¸Ð·Ð½ÐµÑÐ° Ð¸ Ñ„Ð¸Ð·Ð»Ð¸Ñ†
    - AI-Ð°Ð³ÐµÐ½Ñ‚Ñ‹ Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸
    - Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð½Ð° Ð±Ð°Ð·Ðµ LLM
    - Ð ÐµÑˆÐµÐ½Ð¸Ñ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ AI

    Data sources:
    - Reddit (AI & startup subreddits)
    - Google Trends (AI keywords)
    - Product Hunt (AI category)
    - Hacker News (AI news)
    - Twitter/X (AI discussions)
    - Telegram channels (AI news RU)
    - Habr (AI articles)

    Process:
    1. Scrape data from AI-focused sources
    2. Filter for AI-related content
    3. Clean and deduplicate
    4. Extract trends using LLM
    5. Score by engagement & AI relevance
    6. Store in database
    """

    def __init__(self, db: Session):
        super().__init__(db, agent_type="trend_scout")
        self.trend_service = TrendService(db)

        # Initialize scrapers
        try:
            self.reddit_scraper = RedditScraper()
        except Exception as e:
            logger.warning(f"Reddit scraper initialization failed: {e}")
            self.reddit_scraper = None

    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute trend discovery

        Input:
            {
                "sources": ["reddit", "google_trends", ...],
                "limit": 100,
                "subreddits": ["SideProject", "startups"],  # for reddit
                "keywords": ["AI", "SaaS"]  # for google trends
            }

        Output:
            {
                "trends_discovered": 156,
                "trends_stored": 142,
                "duplicates_filtered": 14,
                "breakdown_by_source": {"reddit": 89, "google_trends": 67}
            }
        """
        sources = input_data.get("sources", ["reddit"])
        limit = input_data.get("limit", 100)

        logger.info(
            "Starting trend discovery",
            sources=sources,
            limit=limit
        )

        trends_discovered = []
        breakdown = {}

        # Process each source
        for source in sources:
            if source == "reddit":
                source_trends = await self._discover_from_reddit(input_data)
            elif source == "google_trends":
                source_trends = await self._discover_from_google_trends(input_data)
            else:
                logger.warning(f"Unsupported source: {source}")
                continue

            trends_discovered.extend(source_trends)
            breakdown[source] = len(source_trends)

        # Store trends in database
        trends_stored = 0
        duplicates_filtered = 0

        for trend_data in trends_discovered:
            # TrendService handles duplicate detection
            trend = self.trend_service.create_trend(trend_data)

            # Check if it was a duplicate (same ID returned)
            existing_count = len([t for t in trends_discovered[:trends_discovered.index(trend_data)]
                                 if t.title == trend_data.title])
            if existing_count > 0:
                duplicates_filtered += 1
            else:
                trends_stored += 1

        output = {
            "trends_discovered": len(trends_discovered),
            "trends_stored": trends_stored,
            "duplicates_filtered": duplicates_filtered,
            "breakdown_by_source": breakdown
        }

        logger.info(
            "Trend discovery completed",
            **output
        )

        return output

    async def _discover_from_reddit(self, input_data: Dict[str, Any]) -> List[TrendCreate]:
        """
        Discover AI-focused trends from Reddit

        Uses PRAW to scrape hot posts from AI-related subreddits
        """
        subreddits = input_data.get("subreddits", AI_SUBREDDITS)
        limit = input_data.get("limit", 100)
        time_filter = input_data.get("time_filter", "week")
        sort = input_data.get("sort", "hot")

        logger.info(
            "Scraping Reddit",
            subreddits=subreddits,
            limit=limit,
            sort=sort,
            time_filter=time_filter
        )

        # Check if Reddit scraper is available
        if not self.reddit_scraper:
            logger.warning("Reddit scraper not available, falling back to LLM generation")
            return await self._generate_reddit_trends_with_llm(input_data)

        try:
            # Scrape Reddit using PRAW
            scraped_posts = await self.reddit_scraper.scrape({
                "subreddits": subreddits,
                "limit": limit,
                "time_filter": time_filter,
                "sort": sort
            })

            # Convert scraped posts to TrendCreate objects
            trends = []
            for post in scraped_posts:
                trend = TrendCreate(
                    title=post["title"],
                    description=post["description"],
                    url=post["url"],
                    source=post["source"],
                    category=post["category"],
                    tags=post["tags"],
                    engagement_score=post["engagement_score"],
                    velocity=post["velocity"],
                    metadata=post["metadata"]
                )
                trends.append(trend)

            logger.info(f"Scraped {len(trends)} trends from Reddit")
            return trends

        except Exception as e:
            logger.error(f"Reddit scraping failed: {e}")
            # Fallback to LLM generation
            logger.warning("Falling back to LLM generation")
            return await self._generate_reddit_trends_with_llm(input_data)

    async def _generate_reddit_trends_with_llm(self, input_data: Dict[str, Any]) -> List[TrendCreate]:
        """
        Generate AI-focused trends using LLM

        ðŸŽ¯ Ð¤ÐžÐšÐ£Ð¡: AI-Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸ÐºÐ¸ Ð¸ Ð°Ð³ÐµÐ½Ñ‚Ñ‹ Ð´Ð»Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼
        """
        subreddits = input_data.get("subreddits", AI_SUBREDDITS)
        limit = input_data.get("limit", 100)

        num_ideas = max(limit // max(len(subreddits), 1), 10)  # At least 10 ideas

        prompt = f"""
        ðŸŽ¯ Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐ¹ {num_ideas} ÐÐšÐ¢Ð£ÐÐ›Ð¬ÐÐ«Ð¥ Ñ‚Ñ€ÐµÐ½Ð´Ð¾Ð² Ð² ÑÑ„ÐµÑ€Ðµ AI-Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸ÐºÐ¾Ð² Ð¸ AI-Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð².

        Ð¢Ñ€ÐµÐ½Ð´Ñ‹ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ñ‹ Ð½Ð° Ð Ð•ÐÐ›Ð¬ÐÐ«Ð¥ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°Ñ…, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°ÑŽÑ‚ÑÑ Ð½Ð° Reddit,
        Hacker News, Product Hunt Ð¸ Ð² AI-ÑÐ¾Ð¾Ð±Ñ‰ÐµÑÑ‚Ð²Ðµ.

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        Ð¢Ð Ð•Ð‘ÐžÐ’ÐÐÐ˜Ð¯ Ðš Ð¢Ð Ð•ÐÐ”ÐÐœ:
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ñ‚Ñ€ÐµÐ½Ð´ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÑŒÑÑ Ðº Ð¾Ð´Ð½Ð¾Ð¹ Ð¸Ð· ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¹:
        1. ðŸ¤– AI-ÐÐ¡Ð¡Ð˜Ð¡Ð¢Ð•ÐÐ¢Ð« - Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸ÐºÐ¸ Ð´Ð»Ñ Ð»ÑŽÐ´ÐµÐ¹
        2. ðŸ”„ AI-ÐÐ“Ð•ÐÐ¢Ð« - Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ñ‹Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹
        3. ðŸ› ï¸ AI-Ð˜ÐÐ¡Ð¢Ð Ð£ÐœÐ•ÐÐ¢Ð« - ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÑƒÑ‚Ð¸Ð»Ð¸Ñ‚Ñ‹
        4. ðŸ“Š AI Ð´Ð»Ñ Ð‘Ð˜Ð—ÐÐ•Ð¡Ð - B2B Ñ€ÐµÑˆÐµÐ½Ð¸Ñ
        5. ðŸ‘¤ AI Ð´Ð»Ñ Ð›Ð˜Ð§ÐÐžÐ“Ðž Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐÐ˜Ð¯ - B2C Ñ€ÐµÑˆÐµÐ½Ð¸Ñ

        Ð¢Ñ€ÐµÐ½Ð´Ñ‹ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð¾Ñ‚Ñ€Ð°Ð¶Ð°Ñ‚ÑŒ Ð Ð•ÐÐ›Ð¬ÐÐ«Ð• Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹:
        - Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸
        - ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ€ÑƒÑ‚Ð¸Ð½Ñ‹
        - ÐŸÑ€Ð¸Ð½ÑÑ‚Ð¸Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹
        - ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸
        - ÐŸÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
        - ÐšÐ¾Ð¼Ð¼ÑƒÐ½Ð¸ÐºÐ°Ñ†Ð¸Ñ

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        Ð¤ÐžÐ ÐœÐÐ¢ ÐžÐ¢Ð’Ð•Ð¢Ð (JSON):
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        {{
          "trends": [
            {{
              "title": "AI-Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð´Ð»Ñ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð»Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸ Ñ„Ð¸Ð½Ð°Ð½ÑÐ°Ð¼Ð¸",
              "description": "ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ñ€Ð°ÑÑ…Ð¾Ð´Ð¾Ð², Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ð¸, Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð· Ð±ÑŽÐ´Ð¶ÐµÑ‚Ð°",
              "category": "ai",
              "tags": ["AI", "fintech", "personal finance", "assistant"],
              "engagement_score": 1500,
              "problem_type": "personal",
              "ai_type": "assistant"
            }}
          ]
        }}

        ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸: ai, saas, fintech, health, education, productivity, automation

        Ð’Ð¡Ð• ÐÐÐ—Ð’ÐÐÐ˜Ð¯ Ð˜ ÐžÐŸÐ˜Ð¡ÐÐÐ˜Ð¯ ÐÐ Ð Ð£Ð¡Ð¡ÐšÐžÐœ Ð¯Ð—Ð«ÐšÐ•!
        """

        response = self.call_llm(
            messages=[
                {
                    "role": "system",
                    "content": """Ð¢Ñ‹ ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð¿Ð¾ AI-Ñ‚Ñ€ÐµÐ½Ð´Ð°Ð¼ Ð¸ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð°Ð¼ Ð½Ð° Ð±Ð°Ð·Ðµ Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚Ð°.

Ð¢Ð²Ð¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° - Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒ Ð Ð•ÐÐ›Ð¬ÐÐ«Ð• Ñ‚Ñ€ÐµÐ½Ð´Ñ‹ Ð² ÑÑ„ÐµÑ€Ðµ AI-Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸ÐºÐ¾Ð² Ð¸ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð².
Ð¤Ð¾ÐºÑƒÑÐ¸Ñ€ÑƒÐ¹ÑÑ Ð½Ð° Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°Ñ…, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ AI Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¼Ð¾Ð¶ÐµÑ‚ Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ.

ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ."""
                },
                {"role": "user", "content": prompt}
            ],
            model="gpt-4o",  # Better model for trend discovery
            temperature=0.7,
            json_mode=True
        )

        import json
        trends_data = json.loads(response["content"])

        # Handle different response formats
        trend_list = trends_data if isinstance(trends_data, list) else trends_data.get("trends", trends_data.get("items", []))

        trends = []
        for trend in trend_list[:limit]:
            trends.append(
                TrendCreate(
                    title=trend["title"],
                    description=trend["description"],
                    source="reddit",
                    category=trend["category"],
                    tags=trend["tags"],
                    engagement_score=trend["engagement_score"],
                    url=f"https://reddit.com/r/{subreddits[0]}/",
                    metadata={
                        "subreddit": subreddits[0],
                        "generated": True  # LLM-generated fallback
                    }
                )
            )

        return trends

    async def _discover_from_google_trends(self, input_data: Dict[str, Any]) -> List[TrendCreate]:
        """
        Discover trends from Google Trends

        Uses pytrends to get trending searches
        """
        keywords = input_data.get("keywords", ["AI", "SaaS", "startup"])
        limit = input_data.get("limit", 50)

        logger.info(f"Analyzing Google Trends: {keywords}")

        # TODO: Implement actual Google Trends analysis with pytrends
        # For now, use LLM to generate example trends

        prompt = f"""
        Generate {limit} trending search queries and business opportunities related to: {', '.join(keywords)}.

        Format as JSON array with objects containing:
        - title: Search trend or business idea
        - description: Why this is trending
        - category: Business category
        - tags: Relevant tags
        - velocity: Trend velocity score (0.0-1.0)

        Focus on emerging trends with business potential.
        """

        response = self.call_llm(
            messages=[
                {"role": "system", "content": "You are a Google Trends analysis expert."},
                {"role": "user", "content": prompt}
            ],
            model="gpt-4o-mini",
            temperature=0.7,
            json_mode=True
        )

        import json
        trends_data = json.loads(response["content"])

        # Handle different response formats
        trend_list = trends_data if isinstance(trends_data, list) else trends_data.get("trends", trends_data.get("items", []))

        trends = []
        for trend in trend_list[:limit]:
            trends.append(
                TrendCreate(
                    title=trend["title"],
                    description=trend["description"],
                    source="google_trends",
                    category=trend.get("category", "tech"),
                    tags=trend.get("tags", []),
                    velocity=trend.get("velocity", 0.5),
                    metadata={
                        "keywords": keywords,
                        "generated": True  # Placeholder until real scraper
                    }
                )
            )

        return trends
